import torch
import torch.nn as nn
import torch.nn.functional as F
from ai.embedding.models import register_model

# --- 1. è½»é‡çº§æ³¨æ„åŠ›æ¨¡å— (ä¸å˜) ---
class SEFusionBlock(nn.Module):
    """
    Squeeze-and-Excitation Block for feature fusion.
    Applies lightweight self-attention to a fused feature vector.
    """
    def __init__(self, input_dim: int, reduction_ratio: int = 16):
        """
        Args:
            input_dim (int): The dimension of the fused input vector.
            reduction_ratio (int): The factor by which to reduce the dimension in the bottleneck MLP.
        """
        super().__init__()
        bottleneck_dim = max(input_dim // reduction_ratio, 4) # ä¿è¯ç“¶é¢ˆå±‚ä¸è‡³äºŽå¤ªå°
        
        self.se_module = nn.Sequential(
            # Squeeze: ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚å°†ç»´åº¦é™ä½Ž
            nn.Linear(input_dim, bottleneck_dim),
            nn.ReLU(inplace=True),
            # Excite: å†ç”¨ä¸€ä¸ªçº¿æ€§å±‚æ¢å¤åˆ°åŽŸå§‹ç»´åº¦
            nn.Linear(bottleneck_dim, input_dim),
            # Sigmoidå°†è¾“å‡ºè½¬æ¢ä¸º0-1ä¹‹é—´çš„æ³¨æ„åŠ›åˆ†æ•°
            nn.Sigmoid()
        )

    def forward(self, x):
        """
        Args:
            x (Tensor): The fused input tensor of shape [batch_size, input_dim].
        
        Returns:
            Tensor: The re-weighted feature tensor of the same shape.
        """
        # xæ˜¯æˆ‘ä»¬çš„èžåˆembedding
        
        # se_module(x) ä¼šè¾“å‡ºæ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ³¨æ„åŠ›æƒé‡
        attention_weights = self.se_module(x)
        
        # å°†åŽŸå§‹ç‰¹å¾ä¸Žæ³¨æ„åŠ›æƒé‡ç›¸ä¹˜ï¼Œè¿›è¡Œé‡æ ‡å®š
        reweighted_features = x * attention_weights
        
        return reweighted_features
    

class ResidualMLPBlock(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate, act=nn.ReLU, use_batchnorm=True):
        super().__init__()
        self.p = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim) if use_batchnorm else nn.Identity(),
            act(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, output_dim)
        )
        # å¦‚æžœè¾“å…¥å’Œè¾“å‡ºç»´åº¦ä¸åŒï¼Œåˆ™éœ€è¦ä¸€ä¸ªè·³è·ƒè¿žæŽ¥çš„çº¿æ€§æŠ•å½±
        self.shortcut = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()

    def forward(self, x):
        residual = self.shortcut(x)
        
        out = self.p(x)
        
        # å°†æ®‹å·®åŠ åˆ°è¾“å‡ºä¸Š
        return out + residual

class LSTMTransformerEncoder(nn.Module):
    def __init__(self, input_dim, lstm_hidden_dim, num_lstm_layers, transformer_hidden_dim, num_heads, dropout_rate):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim, num_lstm_layers, batch_first=True, dropout=dropout_rate)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=lstm_hidden_dim, nhead=num_heads, dim_feedforward=transformer_hidden_dim, dropout=dropout_rate, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=1)  # å¯ä»¥è°ƒæ•´å±‚æ•°
        self.lstm_hidden_dim = lstm_hidden_dim
        self.transformer_hidden_dim = transformer_hidden_dim
        self.lstm_to_transformer = nn.Linear(lstm_hidden_dim, transformer_hidden_dim) # çº¿æ€§æ˜ å°„å±‚
    def forward(self, x):
        # LSTM éƒ¨åˆ†
        lstm_output, (h_n, c_n) = self.lstm(x)
        # å°† LSTM çš„è¾“å‡ºä¼ é€’åˆ° Transformer
        # ä½¿ç”¨çº¿æ€§å±‚å°† LSTM çš„éšè—çŠ¶æ€æ˜ å°„åˆ° Transformer çš„ç»´åº¦
        transformer_input = self.lstm_to_transformer(lstm_output)
        # Transformer éƒ¨åˆ†
        transformer_output = self.transformer_encoder(transformer_input)
        # è¿”å›ž Transformer çš„è¾“å‡ºå’Œ LSTM çš„éšè—çŠ¶æ€
        return transformer_output, (h_n, c_n)
    
# --- 3. å·ç§¯æ± åŒ–å±‚ ---
class ConvolutionalFinalLayer(nn.Module):
    def __init__(self, input_dim, embedding_dim, num_filters, kernel_sizes, pool_size, dropout_rate):
        super().__init__()
        self.conv_layers = nn.ModuleList([
            nn.Conv1d(input_dim, num_filters, kernel_size) for kernel_size in kernel_sizes
        ])
        self.pool_layers = nn.ModuleList([
            nn.MaxPool1d(pool_size) for _ in kernel_sizes
        ])
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(num_filters, embedding_dim)

    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        x = x.transpose(1, 2)  # Conv1d éœ€è¦ (batch_size, input_dim, seq_len)
        conv_outputs = []
        for conv, pool in zip(self.conv_layers, self.pool_layers):
            conv_out = F.relu(conv(x))
            pooled_out = pool(conv_out)
            conv_outputs.append(pooled_out)
        # å°†æ‰€æœ‰å·ç§¯æ± åŒ–åŽçš„ç»“æžœæ‹¼æŽ¥èµ·æ¥
        concatenated = torch.cat(conv_outputs, dim=-1)
        concatenated = F.adaptive_max_pool1d(concatenated, 1).squeeze(-1)
        # åº”ç”¨ Dropout
        concatenated = self.dropout(concatenated)
        # ä½¿ç”¨å…¨è¿žæŽ¥å±‚è¿›è¡Œæ˜ å°„
        embedding = self.fc(concatenated)
        return embedding
    
# --- 2. MultiModalAutoencoder (å¸¦æ³¨æ„åŠ› & å¼ºåŒ–é¢„æµ‹ & Batch Norm) ---
@register_model(name='transformer-ae')
class MultiModalAutoencoder(nn.Module):
    def __init__(self, config):
        super().__init__()

        # ä»Žé…ç½®ä¸­åŠ è½½å‚æ•°
        self.ts_input_dim = config['ts_input_dim']
        self.ctx_input_dim = config['ctx_input_dim']
        self.ts_embedding_dim = config['ts_embedding_dim']
        self.ctx_embedding_dim = config['ctx_embedding_dim']
        self.hidden_dim = config['hidden_dim']
        self.num_layers = config['num_layers']
        self.predict_dim = config['predict_dim']
        self.num_heads = config['num_heads']
        self.dropout_rate = config['dropout']
        self.noise_level = config['noise_level']
        self.noise_prob = config['noise_prob']
        self.total_embedding_dim = self.ts_embedding_dim + self.ctx_embedding_dim
        # å®šä¹‰æ¨¡åž‹ç»„ä»¶
        self.ts_encoder = LSTMTransformerEncoder(self.ts_input_dim, self.hidden_dim, self.num_layers, self.hidden_dim, self.num_heads, 0.)
        self.conv_final = ConvolutionalFinalLayer(self.hidden_dim, self.ts_embedding_dim, self.hidden_dim, [3, 5], 2, self.dropout_rate)  # å·ç§¯æ± åŒ–å±‚
        self.ctx_encoder = ResidualMLPBlock(self.ctx_input_dim, self.hidden_dim, self.ctx_embedding_dim, dropout_rate=0, use_batchnorm=True)  # ä¸Šä¸‹æ–‡ç¼–ç å™¨
        self.embedding_norm = nn.LayerNorm(self.total_embedding_dim)
        self.fusion_block = SEFusionBlock(input_dim=self.total_embedding_dim, reduction_ratio=8)
        self.ts_decoder_fc = nn.Linear(self.ts_embedding_dim, self.hidden_dim)
        self.ts_decoder = nn.LSTM(self.hidden_dim, self.hidden_dim, self.num_layers, 
                                  batch_first=True)
        self.ts_output_layer = ResidualMLPBlock(self.hidden_dim, self.hidden_dim, self.ts_input_dim, act=nn.GELU, dropout_rate=self.dropout_rate)
        self.ctx_decoder = ResidualMLPBlock(self.ctx_embedding_dim, self.hidden_dim, self.ctx_input_dim, dropout_rate=self.dropout_rate)  # ä¸Šä¸‹æ–‡è§£ç å™¨
        self.predictor = ResidualMLPBlock(self.total_embedding_dim, int(self.hidden_dim), self.predict_dim, dropout_rate=self.dropout_rate)

        # åˆå§‹åŒ–é¢„æµ‹å¤´
        self.initialize_prediction_head(self.predictor.p[-1])


    def initialize_prediction_head(self, module):
        """
        Initializes the final layer of the predictor to output zero.
        This helps the model start with a strong baseline (predicting zero return).
        """
        print("ðŸ§  Initializing prediction head for faster convergence...")
        if isinstance(module, nn.Linear):
            nn.init.zeros_(module.weight)
            nn.init.zeros_(module.bias)
            print(f"   -> Linear layer {module} has been zero-initialized.")
        else:
            print(f"   -> Module {type(module)} is not a Linear layer, skipping zero-initialization.")


    def forward(self, x_ts, x_ctx):
        _, seq_len, _ = x_ts.shape
        if self.training and self.noise_level > 0:
            if torch.rand(1).item() < self.noise_prob:
                # æ·»åŠ å™ªå£°
                # è¿™é‡Œå‡è®¾ x_ts æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size, seq_len, feature_dim) çš„å¼ é‡
                # å¦‚æžœ x_ts æ˜¯ä¸€ä¸ªä¸€ç»´æ—¶é—´åºåˆ—ï¼Œåˆ™éœ€è¦è°ƒæ•´å™ªå£°çš„å½¢çŠ¶
                noise = torch.normal(0, self.noise_level, size=x_ts.size(), device=x_ts.device)
                x_ts = x_ts + noise
            if torch.rand(1).item() < self.noise_prob:
                # æ·»åŠ å™ªå£°
                noise = torch.normal(0, self.noise_level, size=x_ctx.size(), device=x_ctx.device)
                x_ctx = x_ctx + noise
                
        # ç¼–ç è¿‡ç¨‹
        ts_encoder_output, (ts_h_n, ts_c_n) = self.ts_encoder(x_ts)
        ts_embedding = self.conv_final(ts_encoder_output)  # ä½¿ç”¨å·ç§¯æ± åŒ–å±‚å¾—åˆ°æœ€ç»ˆçš„ embedding
        ctx_embedding = self.ctx_encoder(x_ctx) # (batch_size, ctx_embedding_dim)
        # åœ¨æ­¤å¤„æ‰©å±• final_embedding ä»¥åŒ¹é…åºåˆ—é•¿åº¦
        #  decoder éœ€è¦åºåˆ—é•¿åº¦çš„è¾“å…¥
        #  ä¾‹å¦‚ï¼Œé‡å¤ final_embedding seq_len æ¬¡
        #  è¿™å‡è®¾è§£ç å™¨éœ€è¦ä¸€ä¸ªå½¢çŠ¶ä¸º (batch_size, seq_len, total_embedding_dim) çš„è¾“å…¥
        #  å¦‚æžœè§£ç å™¨åªéœ€è¦ä¸€ä¸ª (batch_size, total_embedding_dim) çš„è¾“å…¥ï¼Œåˆ™ä¸éœ€è¦æ­¤æ­¥éª¤
        #æ‰©å±•final_embeddingï¼Œåˆ›å»ºseq_lenç»´åº¦
        # è§£ç è¿‡ç¨‹
        ts_output = self.ts_decoder_fc(ts_embedding).unsqueeze(1).repeat(1, x_ts.size(1), 1)
        ts_output, _ = self.ts_decoder(ts_output)  #  <--- è§£ç å™¨è¾“å…¥æ˜¯ final_embedding
        ts_output = self.ts_output_layer(ts_output)
        ctx_output = self.ctx_decoder(ctx_embedding)
        # é¢„æµ‹åˆ†æ”¯

        # èžåˆ Embedding
        final_embedding = torch.cat([ts_embedding, ctx_embedding], dim=1).detach()  # (batch_size, total_embedding_dim)
        final_embedding = self.embedding_norm(final_embedding)
        final_embedding = self.fusion_block(final_embedding) # (batch_size, total_embedding_dim)

        predict_output = self.predictor(final_embedding)
        return ts_output, ctx_output, predict_output, final_embedding

    def get_embedding(self, x_ts, x_ctx):
        """ç”¨äºŽæŽ¨ç†çš„å‡½æ•°ï¼Œåªè¿”å›žèžåˆåŽçš„embeddingã€‚"""
        with torch.no_grad():
            ts_encoder_outputs, (ts_h_n, ts_c_n) = self.ts_encoder(x_ts)
            ts_last_hidden_state = ts_h_n[-1, :, :]
            ts_embedding = self.ts_encoder_fc(ts_last_hidden_state)
            ctx_embedding = self.ctx_encoder(x_ctx)
            final_embedding = torch.cat([ts_embedding, ctx_embedding], dim=1)
        return final_embedding